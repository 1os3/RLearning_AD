import os
import time
import argparse
import yaml
import numpy as np
import torch
import random
import gc
import os
import csv
from pathlib import Path
from datetime import datetime

from src.environment.airsim_env import AirSimDroneEnv as AirSimUAVEnvironment
from src.models.policy_module.shared_trunk import ActorCriticNetwork
from src.algorithms.sac import SAC
from src.utils.logger import Logger
from src.utils.evaluator import Evaluator
from src.utils.resource_monitor import ResourceMonitor
from src.utils.memory_manager import MemoryManager

def set_seed(seed=None):
    """Set all random seeds for reproducibility.
    
    Args:
        seed: Random seed to use. If None, uses 42 as default.
    """
    # ç§å­æ°¸è¿œä¸åº”è¯¥æ˜¯Noneï¼Œå› ä¸ºæˆ‘ä»¬åœ¨mainä¸­å·²å¤„ç†é»˜è®¤å€¼
    # è¿™é‡Œä½œä¸ºå®‰å…¨æ£€æŸ¥ï¼Œä»ç„¶æä¾›é»˜è®¤å€¼
    if seed is None:
        seed = 42
        print(f"è­¦å‘Š: æœªèƒ½æ­£ç¡®è¯»å–éšæœºç§å­ï¼Œä½¿ç”¨é»˜è®¤å€¼ {seed}")
    else:
        print(f"è®¾ç½®éšæœºç§å­: {seed}")
    
    # è®¾ç½®æ‰€æœ‰éšæœºç§å­
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    
    # å¯¹XPUè®¾å¤‡ä¹Ÿåšç›¸åŒå¤„ç†ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if hasattr(torch, 'xpu') and torch.xpu.is_available():
        torch.xpu.manual_seed(seed)
        torch.xpu.manual_seed_all(seed)
    
    print(f"å·²è®¾ç½®æ‰€æœ‰éšæœºç§å­ä¸º: {seed}")

def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(description="Train UAV RL Controller")
    parser.add_argument("--config", type=str, default="config/training.yaml", 
                        help="Path to config file (default: config/training.yaml)")
    parser.add_argument("--seed", type=int, default=None, 
                        help="Random seed (overrides config file)")
    parser.add_argument("--device", type=str, default=None, 
                        help="Device to use (auto, cuda, xpu, cpu, overrides config file)")
    parser.add_argument("--checkpoint", type=str, default=None, 
                        help="Path to checkpoint to resume from")
    parser.add_argument("--checkpoint_version", type=str, default="latest", 
                        help="Checkpoint version to load (latest or timestamp)")
    parser.add_argument("--eval_only", action="store_true", 
                        help="Run evaluation only (no training)")
    parser.add_argument("--log_dir", type=str, default=None, 
                        help="Directory to save logs (overrides config file)")
    parser.add_argument("--save_dir", type=str, default=None, 
                        help="Directory to save checkpoints (overrides config file)")
    parser.add_argument("--save_freq", type=int, default=None, 
                        help="Save checkpoint every N steps (overrides config file)")
    parser.add_argument("--eval_freq", type=int, default=None, 
                        help="Run evaluation every N steps (overrides config file)")
    parser.add_argument("--eval_episodes", type=int, default=None, 
                        help="Number of episodes for evaluation")
    parser.add_argument("--total_steps", type=int, default=10000000, 
                        help="Total number of training steps")
    parser.add_argument("--airsim_client_timeout", type=float, default=60.0,
                        help="AirSim client connection timeout in seconds")
    
    return parser.parse_args()

def load_config_with_inheritance(config_path):
    """Load YAML configuration with inheritance support.
    
    Supports _inherit field in YAML files to inherit from another config file.
    Child config values override parent config values.
    
    Args:
        config_path: Path to the main config file
        
    Returns:
        Merged configuration dictionary
        
    Raises:
        FileNotFoundError: å¦‚æœé…ç½®æ–‡ä»¶æˆ–çˆ¶é…ç½®æ–‡ä»¶ä¸å­˜åœ¨
        yaml.YAMLError: å¦‚æœYAMLæ ¼å¼æ— æ•ˆ
    """
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            try:
                config = yaml.safe_load(f)
                if config is None:  # ç©ºæ–‡ä»¶æˆ–æ ¼å¼é”™è¯¯
                    print(f"è­¦å‘Šï¼šé…ç½®æ–‡ä»¶ {config_path} ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯ï¼Œä½¿ç”¨ç©ºå­—å…¸")
                    config = {}
            except yaml.YAMLError as e:
                raise yaml.YAMLError(f"é…ç½®æ–‡ä»¶ {config_path} YAMLæ ¼å¼é”™è¯¯: {str(e)}")
    except FileNotFoundError:
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ {config_path} ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥è·¯å¾„")
    
    # Process inheritance if specified
    if '_inherit' in config:
        try:
            parent_path = os.path.join(os.path.dirname(config_path), config['_inherit'])
            try:
                with open(parent_path, 'r', encoding='utf-8') as f:
                    try:
                        parent_config = yaml.safe_load(f)
                        if parent_config is None:  # ç©ºæ–‡ä»¶æˆ–æ ¼å¼é”™è¯¯
                            print(f"è­¦å‘Šï¼šçˆ¶é…ç½®æ–‡ä»¶ {parent_path} ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯ï¼Œä½¿ç”¨ç©ºå­—å…¸")
                            parent_config = {}
                    except yaml.YAMLError as e:
                        raise yaml.YAMLError(f"çˆ¶é…ç½®æ–‡ä»¶ {parent_path} YAMLæ ¼å¼é”™è¯¯: {str(e)}")
            except FileNotFoundError:
                raise FileNotFoundError(f"çˆ¶é…ç½®æ–‡ä»¶ {parent_path} ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥è·¯å¾„æˆ–_inheritå€¼ '{config['_inherit']}'")
            
            # Remove inheritance key from config
            inherit_key = config.pop('_inherit')
            
            # Deep merge the configs (parent serves as base, child overrides)
            merged_config = deep_merge_configs(parent_config, config)
            print(f"å·²åŠ è½½é…ç½® {config_path} (ç»§æ‰¿è‡ª {inherit_key})")
            return merged_config
        except Exception as e:
            print(f"å¤„ç†é…ç½®ç»§æ‰¿æ—¶å‡ºé”™: {str(e)}")
            raise
    else:
        print(f"å·²åŠ è½½é…ç½® {config_path}")
        return config

def deep_merge_configs(base, override):
    """Deep merge two configuration dictionaries.
    
    Values from override dict will take precedence over base dict.
    
    Args:
        base: Base configuration (parent)
        override: Override configuration (child)
        
    Returns:
        Merged configuration dictionary
    """
    result = base.copy()
    
    for key, value in override.items():
        if key in result and isinstance(result[key], dict) and isinstance(value, dict):
            # Recursively merge nested dictionaries
            result[key] = deep_merge_configs(result[key], value)
        else:
            # Override or add values
            result[key] = value
    
    return result

def select_device(device_arg):
    """Select appropriate device based on arguments and availability."""
    if device_arg == "auto":
        # Try to use XPU first (Intel GPU), then CUDA, then CPU
        if torch.xpu.is_available():
            return torch.device("xpu:0")
        elif torch.cuda.is_available():
            return torch.device("cuda:0")
        else:
            return torch.device("cpu")
    elif device_arg == "xpu":
        if torch.xpu.is_available():
            return torch.device("xpu:0")
        else:
            print("XPU requested but not available. Falling back to CPU.")
            return torch.device("cpu")
    elif device_arg == "cuda":
        if torch.cuda.is_available():
            return torch.device("cuda:0")
        else:
            print("CUDA requested but not available. Falling back to CPU.")
            return torch.device("cpu")
    else:
        return torch.device("cpu")

def main():
    """Main training function."""
    # Parse arguments
    args = parse_args()
    
    # Load config with inheritance support
    config = load_config_with_inheritance(args.config)
    
    # ç¡®å®šè¦ä½¿ç”¨çš„ç§å­: å‘½ä»¤è¡Œå‚æ•°ä¼˜å…ˆï¼Œå…¶æ¬¡æ˜¯é…ç½®æ–‡ä»¶ï¼Œæœ€åæ˜¯é»˜è®¤å€¼
    seed = args.seed if args.seed is not None else config.get('general', {}).get('seed', 42)
    
    # è®¾ç½®éšæœºç§å­
    set_seed(seed)
    
    # Update config with command line arguments (only if values are provided)
    if args.total_steps is not None:
        config['training']['total_steps'] = args.total_steps
    # å·²ç»å¤„ç†è¿‡ç§å­ï¼Œç¡®ä¿é…ç½®æ–‡ä»¶ä¸ä½¿ç”¨çš„ç§å­ä¸€è‡´
    config['general']['seed'] = seed
    if args.save_freq is not None:
        config['training']['save_interval'] = args.save_freq
    if args.eval_freq is not None:
        config['training']['eval_interval'] = args.eval_freq
    if args.eval_episodes is not None:
        config['training']['eval_episodes'] = args.eval_episodes
    if args.airsim_client_timeout is not None:
        config['environment']['airsim']['timeout_ms'] = int(args.airsim_client_timeout * 1000)
    if args.device is not None:
        config['general']['device'] = args.device
    if args.log_dir is not None:
        config['general']['log_dir'] = args.log_dir
    if args.save_dir is not None:
        config['general']['checkpoint_dir'] = args.save_dir
    
    # Setup device
    # å½“å‘½ä»¤è¡Œå‚æ•°æœªæŒ‡å®šæ—¶ï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è®¾å¤‡è®¾ç½®
    device_setting = args.device if args.device is not None else config['general'].get('device', 'auto')
    device = select_device(device_setting)
    print(f"Using device: {device}")
    
    # Setup directories
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    
    # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é»˜è®¤å€¼ï¼Œå¦‚æœå‘½ä»¤è¡Œå‚æ•°ä¸ºNone
    log_dir_base = args.log_dir if args.log_dir is not None else config['general'].get('log_dir', 'logs')
    save_dir_base = args.save_dir if args.save_dir is not None else config['general'].get('checkpoint_dir', 'checkpoints')
    
    # ç¡®ä¿ç›®å½•è·¯å¾„å­˜åœ¨
    if not os.path.exists(log_dir_base):
        os.makedirs(log_dir_base, exist_ok=True)
        print(f"åˆ›å»ºæ—¥å¿—ç›®å½•: {log_dir_base}")
    
    if not os.path.exists(save_dir_base):
        os.makedirs(save_dir_base, exist_ok=True)
        print(f"åˆ›å»ºæ£€æŸ¥ç‚¹ç›®å½•: {save_dir_base}")
    
    # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„å­ç›®å½•
    log_dir = os.path.join(log_dir_base, timestamp)
    save_dir = os.path.join(save_dir_base, timestamp)
    os.makedirs(log_dir, exist_ok=True)
    os.makedirs(save_dir, exist_ok=True)
    
    print(f"è®­ç»ƒæ—¥å¿—å­˜å‚¨åˆ°: {log_dir}")
    print(f"æ¨¡å‹æ£€æŸ¥ç‚¹å­˜å‚¨åˆ°: {save_dir}")
    
    # Save config
    with open(os.path.join(log_dir, 'config.yaml'), 'w') as f:
        yaml.dump(config, f)
    
    # Setup logger
    logger = Logger(log_dir)
    
    # èµ„æºç›‘æ§å·²ç¦ç”¨
    # resource_monitor = ResourceMonitor(logger=logger, interval=5)
    # resource_monitor.start()
    
    # åˆå§‹åŒ–å†…å­˜ç®¡ç†å™¨
    # å°†å†…å­˜æ£€æŸ¥é¢‘ç‡é™ä½ä¸º100æ­¥ä¸€æ¬¡ï¼Œè€Œéé»˜è®¤çš„1000æ­¥
    memory_check_interval = 100  # ç›´æ¥è®¾ç½®ä¸º100æ­¥ï¼Œä¸ä½¿ç”¨é…ç½®å€¼
    memory_threshold = config.get('memory_management', {}).get('emergency_threshold', 0.85)
    memory_manager = MemoryManager(
        device=device,
        check_interval=memory_check_interval,
        emergency_threshold=memory_threshold,
        clear_cuda_cache=True,
        debug_mode=config.get('debug', False)
    )
    print(f"å†…å­˜ç®¡ç†å™¨å·²åˆå§‹åŒ–: æ£€æŸ¥é—´éš”={memory_check_interval}æ­¥, é˜ˆå€¼={memory_threshold*100:.1f}%")
    
    # Create environment
    env = AirSimUAVEnvironment(config['environment'])
    eval_env = AirSimUAVEnvironment(config['environment'], evaluation=True)
    
    # Setup evaluator
    evaluator = Evaluator(
        env=eval_env,
        num_episodes=args.eval_episodes,
        render=config['evaluation'].get('render', False),
        record=config['evaluation'].get('record', False),
        record_dir=os.path.join(log_dir, 'videos')
    )
    
    # Create agent
    agent = SAC(config, device)
    
    # è·å–æ€»è®­ç»ƒæ­¥æ•°ï¼Œä½¿å…¶åœ¨æ•´ä¸ªå‡½æ•°ä¸­å¯ç”¨
    total_steps = config['training'].get('total_steps', config['training'].get('num_steps', 1000000))
    
    # å…ˆæ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°æ˜¯å¦æŒ‡å®šäº†æ£€æŸ¥ç‚¹
    checkpoint_loaded = False
    
    # Load checkpoint if specified via command line
    if args.checkpoint is not None:
        agent.load(args.checkpoint, args.checkpoint_version)
        print(f"\nâœ”ï¸ å·²æˆåŠŸä»å‘½ä»¤è¡Œå‚æ•°åŠ è½½æ£€æŸ¥ç‚¹: {args.checkpoint} (version {args.checkpoint_version})")
        checkpoint_loaded = True
    # å¦‚æœå‘½ä»¤è¡Œå‚æ•°æ²¡æœ‰æŒ‡å®šæ£€æŸ¥ç‚¹ï¼Œä½†é…ç½®æ–‡ä»¶å¼€å¯äº†æ–­ç‚¹ç»­è®­
    elif config['training'].get('resume_training', False) and config['training'].get('checkpoint_path', ''):
        checkpoint_path = config['training'].get('checkpoint_path', '')
        if os.path.exists(checkpoint_path):
            # å°è¯•åŠ è½½æ£€æŸ¥ç‚¹
            try:
                agent.load(os.path.dirname(checkpoint_path), os.path.basename(checkpoint_path).replace('ac_network_', '').replace('.pt', ''))
                print(f"\nâœ”ï¸ å·²æˆåŠŸä»é…ç½®æ–‡ä»¶åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")
                print(f"  - ç®—æ³•è¿­ä»£æ­¥æ•°: {agent.updates}")
                print(f"  - ç´¯è®¡è®­ç»ƒæ­¥æ•°: {agent.total_steps}")
                print(f"  - ç´¯è®¡è®­ç»ƒå›åˆ: {agent.episodes}")
                print(f"  - å·²å®Œæˆè®­ç»ƒè¿›åº¦: {(agent.total_steps / total_steps) * 100:.2f}%")
                checkpoint_loaded = True
            except Exception as e:
                print(f"\nâš ï¸ åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {checkpoint_path}")
                print(f"  é”™è¯¯ä¿¡æ¯: {str(e)}")
        else:
            print(f"\nâš ï¸ æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
    
    if not checkpoint_loaded:
        print("\nğŸ“¢ åˆ›å»ºæ–°æ¨¡å‹ï¼Œä»å¤´å¼€å§‹è®­ç»ƒ")
    
    # Evaluation only mode
    if args.eval_only:
        print("Running evaluation only...")
        eval_results = evaluator.evaluate(agent)
        for key, value in eval_results.items():
            print(f"{key}: {value}")
        return
    
    # è®­ç»ƒå¾ªç¯å…¶ä»–å˜é‡
    save_freq = config['training'].get('save_freq', config['training'].get('save_interval', 10000))
    eval_freq = config['training'].get('eval_freq', config['training'].get('eval_interval', 5000))
    initial_step = agent.total_steps  # For resuming from checkpoint
    episode_rewards = []
    episode_lengths = []
    current_episode_reward = 0
    current_episode_length = 0
    episode_count = agent.episodes
    
    # Reset environment - Gymnasium APIè¿”å›(observation, info)å…ƒç»„
    state, _ = env.reset()
    
    # Main training loop
    print(f"Starting training for {total_steps} steps...")
    for step in range(initial_step, total_steps):
        # Select action from policy
        if step < config['training'].get('start_steps', 1000):
            # Random actions for initial exploration
            action = env.action_space.sample()
        else:
            # Sample action from policy
            action = agent.select_action(state)
        
        # Take step in environment
        # æ–°ç‰ˆGymnasium APIè¿”å›5ä¸ªå€¼ï¼š(observation, reward, terminated, truncated, info)
        next_state, reward, terminated, truncated, info = env.step(action)
        # å°†terminatedå’Œtruncatedåˆå¹¶ä¸ºdoneæ ‡å¿—
        done = terminated or truncated
        
        # æ¯éš”ä¸€å®šæ­¥æ•°è®°å½•è¯¦ç»†å¥–åŠ±ç»„ä»¶
        if step % 1000 == 0 and 'reward_components' in info:
            reward_components = info['reward_components']
            for key, value in reward_components.items():
                logger.log_scalar(f"reward/{key}", value, step)
            
            # åœ¨æ§åˆ¶å°ä¸­æ‰“å°å¥–åŠ±ç»„ä»¶
            if step % 5000 == 0:  # æ¯5000æ­¥æ‰“å°ä¸€æ¬¡è¯¦ç»†ä¿¡æ¯
                print(f"\nå¥–åŠ±ç»„ä»¶ (Step {step}):")
                for key, value in reward_components.items():
                    print(f"  - {key}: {value:.4f}")
        
        # Store transition in replay buffer
        # Convert to tensors - æ³¨æ„å¤„ç†å­—å…¸ç±»åobservation
        # æ£€æŸ¥stateæ˜¯å¦ä¸ºå­—å…¸ç±»åï¼ˆç¯å¢ƒè¿”å›çš„å¤åˆè§‚æµ‹ï¼‰
        if isinstance(state, dict):
            # å¤„ç†å­—å…¸ç±»åè§‚æµ‹ - å°†å„ç»„ä»¶è½¬æ¢ä¸ºtensor
            state_dict = {}
            for key, value in state.items():
                if key == 'image':
                    # å›¾åƒæ•°æ®éœ€è¦è½¬æ¢ä¸ºfloatå¹¶è§„èŒƒåŒ–
                    if value is not None:
                        state_dict[key] = torch.FloatTensor(value).to(device) / 255.0  # è§„èŒƒåŒ–åˆ°[0,1]
                    else:
                        state_dict[key] = None
                else:
                    # å…¶ä»–ç±»åçš„æ•°æ®ï¼ˆstateã€targetç­‰ï¼‰
                    state_dict[key] = torch.FloatTensor(value).to(device)
            state_tensor = state_dict
        else:
            # å¦‚æœä¸æ˜¯å­—å…¸ï¼Œç›´æ¥è½¬æ¢ï¼ˆå…¼å®¹æ€§å¤„ç†ï¼‰
            state_tensor = torch.FloatTensor(state).to(device)
        
        # åŒæ ·å¤„ç†next_state
        if isinstance(next_state, dict):
            next_state_dict = {}
            for key, value in next_state.items():
                if key == 'image':
                    # å›¾åƒæ•°æ®éœ€è¦è½¬æ¢ä¸ºfloatå¹¶è§„èŒƒåŒ–
                    if value is not None:
                        next_state_dict[key] = torch.FloatTensor(value).to(device) / 255.0
                    else:
                        next_state_dict[key] = None
                else:
                    next_state_dict[key] = torch.FloatTensor(value).to(device)
            next_state_tensor = next_state_dict
        else:
            next_state_tensor = torch.FloatTensor(next_state).to(device)
        
        # å…¶ä»–æ•°æ®è½¬æ¢
        action_tensor = torch.FloatTensor(action).to(device)
        reward_tensor = torch.FloatTensor([reward]).to(device)
        done_tensor = torch.FloatTensor([float(done)]).to(device)
        
        # Add to buffer
        agent.replay_buffer.push(
            state_tensor, 
            action_tensor, 
            reward_tensor, 
            next_state_tensor, 
            done_tensor
        )
        
        # Update current episode stats
        current_episode_reward += reward
        current_episode_length += 1
        
        # Update agent
        if step >= config['training'].get('update_after', 1000):
            update_metrics = agent.train(step)
            
            # æ—¥å¿—æ›´æ–°æŒ‡æ ‡å¹¶æ‰“å°è¯¦ç»†è®­ç»ƒä¿¡æ¯
            if update_metrics:
                # æ‰“å°è®­ç»ƒæ­¥éª¤è¯¦ç»†ä¿¡æ¯
                print(f"Step {step}/{total_steps} (è¿›åº¦: {(step/total_steps)*100:.2f}%) | ", end="")
                metric_strs = []
                for key, value in update_metrics.items():
                    logger.log_scalar(f"train/{key}", value, step)
                    metric_strs.append(f"{key}: {value:.4f}")
                print(f"Loss: {' | '.join(metric_strs)} | æ¨¡å‹æ­£åœ¨å­¦ä¹ ...")
                
                # å®šæœŸæ‰§è¡Œå†…å­˜ç®¡ç†
                cleaned, mem_stats = memory_manager.check_and_collect(step, logger)
                if cleaned:
                    # è®°å½•å†…å­˜æ¸…ç†ä¿¡æ¯
                    logger.log_scalar("memory/cleanup_triggered", 1.0, step)
                    if mem_stats.get('system_ram_percent', 0) > 0:
                        print(f"âš ï¸ å†…å­˜æ¸…ç†: ç³»ç»Ÿå†…å­˜ä½¿ç”¨ç‡ {mem_stats.get('system_ram_percent', 0)*100:.1f}%, " +
                              f"{'CUDA' if mem_stats.get('gpu_used', 0) > 0 else 'XPU' if mem_stats.get('xpu_used', 0) > 0 else ''}" +
                              f"æ˜¾å­˜ä½¿ç”¨ {mem_stats.get('gpu_used', mem_stats.get('xpu_used', 0)):.2f}GB")
        
        # Handle episode termination
        if done:
            # Reset environment - Gymnasium APIè¿”å›(observation, info)å…ƒç»„
            state, _ = env.reset()
            
            # Log episode results
            episode_rewards.append(current_episode_reward)
            episode_lengths.append(current_episode_length)
            logger.log_scalar("train/episode_reward", current_episode_reward, step)
            logger.log_scalar("train/episode_length", current_episode_length, step)
            
            # è®°å½•æœ€åä¸€æ­¥çš„å¥–åŠ±è¯¦æƒ…åˆ°æ—¥å¿—
            print(f"\nå¥–åŠ±è¯¦æƒ… (Episode {episode_count}): æ€»å¥–åŠ±={current_episode_reward:.4f}")
            
            # å¦‚æœè¿˜æœ‰æœ€åä¸€æ­¥çš„å¥–åŠ±ç»„ä»¶ä¿¡æ¯ï¼Œä¹Ÿè¾“å‡ºå‡ºæ¥
            if 'reward_components' in info:
                reward_components = info['reward_components']
                for key, value in reward_components.items():
                    print(f"  - {key}: {value:.4f}")
                    
            reward_csv_data = {"step": step, "episode": episode_count, "total_reward": current_episode_reward}
            
            # Reset episode stats
            current_episode_reward = 0
            current_episode_length = 0
            episode_count += 1
            agent.episodes = episode_count
            
            # Print episode summary
            if episode_count % 10 == 0:
                mean_reward = np.mean(episode_rewards[-10:])
                mean_length = np.mean(episode_lengths[-10:])
                print(f"Episode {episode_count}, Step {step}/{total_steps}, "
                      f"Mean Reward: {mean_reward:.2f}, Mean Length: {mean_length:.2f}")
                
                # å°†å¥–åŠ±ç»Ÿè®¡æ•°æ®ä¿å­˜åˆ°CSV
                try:
                    # ç¡®ä¿å¥–åŠ±æ—¥å¿—ç›®å½•å­˜åœ¨
                    reward_log_dir = os.path.join(log_dir, 'reward_logs')
                    os.makedirs(reward_log_dir, exist_ok=True)
                    
                    # å¥–åŠ±ç»Ÿè®¡CSVæ–‡ä»¶è·¯å¾„
                    reward_stats_path = os.path.join(reward_log_dir, 'reward_stats.csv')
                    
                    # å®šä¹‰æˆ–æ›´æ–°CSVå¤´
                    header = ['episode', 'step', 'mean_reward', 'mean_length']
                    row = [episode_count, step, mean_reward, mean_length]
                    
                    # å†™å…¥CSV
                    if not os.path.exists(reward_stats_path):
                        with open(reward_stats_path, 'w', newline='') as f:
                            writer = csv.writer(f)
                            writer.writerow(header)
                            writer.writerow(row)
                    else:
                        with open(reward_stats_path, 'a', newline='') as f:
                            writer = csv.writer(f)
                            writer.writerow(row)
                except Exception as e:
                    print(f"è­¦å‘Š: ä¿å­˜å¥–åŠ±ç»Ÿè®¡æ•°æ®æ—¶å‡ºé”™: {e}")
        else:
            # Continue to next step
            state = next_state
        
        # Save checkpoint
        if (step + 1) % save_freq == 0:
            # åœ¨ä¿å­˜æ¨¡å‹å‰æ‰§è¡Œå†…å­˜å›æ”¶ï¼Œé¿å…ä¸å¿…è¦çš„ä¸´æ—¶å¯¹è±¡å ç”¨ç©ºé—´
            memory_manager.check_and_collect(step, logger, force=True)
            
            try:
                agent.save(save_dir)
                print(f"Saved checkpoint at step {step + 1}")
            except Exception as e:
                print(f"âš ï¸ ä¿å­˜æ¨¡å‹æ—¶å‡ºé”™: {e}")
                # å‡ºç°é”™è¯¯æ—¶ç´§æ€¥æ¸…ç†å†…å­˜
                memory_manager.emergency_cleanup(logger, step)
        
        # Run evaluation
        if (step + 1) % eval_freq == 0:
            # åœ¨è¯„ä¼°å‰æ‰§è¡Œå†…å­˜å›æ”¶ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿç©ºé—´æ‰§è¡Œè¯„ä¼°
            memory_manager.check_and_collect(step, logger, force=True)
            
            try:
                print(f"Running evaluation at step {step + 1}...")
                eval_results = evaluator.evaluate(agent)
                
                # Log evaluation results
                for key, value in eval_results.items():
                    logger.log_scalar(f"eval/{key}", value, step)
            except Exception as e:
                print(f"âš ï¸ è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
                # å‡ºç°é”™è¯¯æ—¶ç´§æ€¥æ¸…ç†å†…å­˜
                memory_manager.emergency_cleanup(logger, step)
            
            print(f"Evaluation at step {step + 1}: Mean Reward: {eval_results['mean_reward']:.2f}")
    
    # èµ„æºç›‘æ§å·²ç¦ç”¨
    # resource_monitor.stop()
    
    # è®­ç»ƒç»“æŸå‰è¿›è¡Œä¸€æ¬¡å…¨é¢å†…å­˜æ¸…ç†
    print("è®­ç»ƒå®Œæˆï¼Œæ‰§è¡Œæœ€ç»ˆå†…å­˜æ¸…ç†...")
    memory_manager.emergency_cleanup(logger, step)
    
    # è®°å½•æœ€ç»ˆå†…å­˜ä½¿ç”¨æƒ…å†µ
    final_mem_stats = memory_manager.get_memory_stats()
    logger.log_scalar("memory/final_system_ram", final_mem_stats.get('system_ram_percent', 0), step)
    if memory_manager.use_cuda:
        logger.log_scalar("memory/final_gpu_used_gb", final_mem_stats.get('gpu_used', 0), step)
    elif memory_manager.use_xpu:
        logger.log_scalar("memory/final_xpu_used_gb", final_mem_stats.get('xpu_used', 0), step)
    
    # Final save and evaluation
    try:
        print("ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
        agent.save(save_dir)
        
        print("æ‰§è¡Œæœ€ç»ˆè¯„ä¼°...")
        eval_results = evaluator.evaluate(agent)
        
        # Print final summary
        print("========== è®­ç»ƒå®Œæˆ ==========")
        print(f"æœ€ç»ˆè¯„ä¼°ç»“æœ: å¹³å‡å¥–åŠ± {eval_results['mean_reward']:.2f}")
        print(f"æ¨¡å‹å·²ä¿å­˜è‡³: {save_dir}")
        print(f"æ—¥å¿—å·²ä¿å­˜è‡³: {log_dir}")
        
        # è®°å½•è®­ç»ƒæ€»ç»“
        logger.log_scalar("final/mean_reward", eval_results['mean_reward'], step)
        logger.log_scalar("final/episode_count", episode_count, step)
        logger.log_scalar("final/training_steps", step, step)
        logger.log_text("final/training_summary", f"è®­ç»ƒå®Œæˆï¼Œå…±æ‰§è¡Œ{step}æ­¥ï¼Œ{episode_count}å›åˆï¼Œæœ€ç»ˆå¹³å‡å¥–åŠ±{eval_results['mean_reward']:.2f}")
    except Exception as e:
        print(f"âš ï¸ æœ€ç»ˆä¿å­˜æˆ–è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
    finally:
        # ç¡®ä¿èµ„æºé‡Šæ”¾
        print("é‡Šæ”¾ç¯å¢ƒèµ„æº...")
        try:
            env.close()
        except Exception as e:
            print(f"å…³é—­è®­ç»ƒç¯å¢ƒå‡ºé”™: {e}")
            
        try:
            eval_env.close() 
        except Exception as e:
            print(f"å…³é—­è¯„ä¼°ç¯å¢ƒå‡ºé”™: {e}")
            
        # æœ€ç»ˆç¡®ä¿æ‰€æœ‰GPU/å†…å­˜èµ„æºé‡Šæ”¾
        memory_manager.collect_garbage(force=True)
        print("å†…å­˜èµ„æºå·²é‡Šæ”¾")

if __name__ == "__main__":
    main()
